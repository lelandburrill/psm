{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82efa1b3-3125-43d1-9f1b-8dd895c7584b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excuted at 2024-01-29 05:10:11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "from datetime import datetime\n",
    "def executed():\n",
    "    #\"%Y-%m-%d %I:%M:%S %p\"\n",
    "    print(\"excuted at\", datetime.now().strftime(\"%Y-%m-%d %I:%M:%S\")) \n",
    "executed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dfaf57-801c-4e2d-aef2-a17119ca989b",
   "metadata": {},
   "source": [
    "BEGIN DATA PREP  \n",
    "BEGIN USER-FACING PARAMETERIZATION STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3f85a7f-27e5-4f88-a657-c67689793706",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = 'CVD.csv' #Pull data from csv for Anaconda development\n",
    "categorical_features = ['x.Gender','Smoke']\n",
    "numeric_features = ['x.Age']\n",
    "binary_response = 'CVD' #Currently expects a 1/0-indicator for treatment/control, respectively\n",
    "member_identifier = 'Unnamed: 0'  \n",
    "exact_match_features = ['x.Gender','Smoke']\n",
    "\n",
    "user_caliper = None #If left 'None', default caliper of 0.2*std.dev(logit) will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a612fad-12a3-4901-87c1-ee317b0e6d69",
   "metadata": {},
   "source": [
    "Default caliper reference:  \n",
    "Zhao QY, Luo JC, Su Y, Zhang YJ, Tu GW, Luo Z.  \n",
    "Propensity score matching with R: conventional methods and new features.  \n",
    "Ann Transl Med 2021;9(9):812.   \n",
    "doi: 10.21037/atm-20-3998    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b756dad-ed21-4005-80c8-b6d7679d0e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 excuted at 2024-01-29 05:10:13\n",
      "Step 2 excuted at 2024-01-29 05:10:13\n",
      "Step 3 excuted at 2024-01-29 05:10:13\n",
      "Step 4 excuted at 2024-01-29 05:10:13\n",
      "Step 5 excuted at 2024-01-29 05:10:13\n",
      "Step 6 excuted at 2024-01-29 05:10:13\n",
      "Step 7 excuted at 2024-01-29 05:10:13\n",
      "Step 8 excuted at 2024-01-29 05:10:13\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Backend set-up\n",
    "covariates = categorical_features + numeric_features\n",
    "df = pd.read_csv(csv)\n",
    "X = df[covariates]\n",
    "y = df[binary_response]\n",
    "print(\"Step 1 excuted at\", datetime.now().strftime(\"%Y-%m-%d %I:%M:%S\"))                  \n",
    "\n",
    "# Step 2: Encode Categorical Variables\n",
    "# Include standardization in the pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "    ])\n",
    "print(\"Step 2 excuted at\", datetime.now().strftime(\"%Y-%m-%d %I:%M:%S\"))                  \n",
    "\n",
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=21)\n",
    "print(\"Step 3 excuted at\", datetime.now().strftime(\"%Y-%m-%d %I:%M:%S\"))                  \n",
    "\n",
    "# Step 4: Build a logistic regression pipeline\n",
    "logreg_model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "print(\"Step 4 excuted at\", datetime.now().strftime(\"%Y-%m-%d %I:%M:%S\"))                  \n",
    "\n",
    "# Step 5: Fit the logistic regression model\n",
    "logreg_model.fit(X_train, y_train)\n",
    "print(\"Step 5 excuted at\", datetime.now().strftime(\"%Y-%m-%d %I:%M:%S\"))                  \n",
    "\n",
    "# Step 6: Extract Propensity Scores for the entire dataset\n",
    "propensity_scores = logreg_model.predict_proba(X)[:, 1]\n",
    "print(\"Step 6 excuted at\", datetime.now().strftime(\"%Y-%m-%d %I:%M:%S\"))                  \n",
    "\n",
    "#step 7: Create dataframe from logreg model and calculate logit \n",
    "result_df = pd.DataFrame({member_identifier: df[member_identifier], 'PropensityScore': propensity_scores})\n",
    "result_df['logit'] = np.log(result_df['PropensityScore'] / (1-result_df['PropensityScore'])) \n",
    "print(\"Step 7 excuted at\", datetime.now().strftime(\"%Y-%m-%d %I:%M:%S\"))                  \n",
    "\n",
    "#Step 8 Merge propensity scores and logit onto starting data set\n",
    "df = pd.merge(df, result_df, on=member_identifier, how='left')\n",
    "df['key']=1\n",
    "print(\"Step 8 excuted at\", datetime.now().strftime(\"%Y-%m-%d %I:%M:%S\"))                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0405d1fc-f991-4572-b3f4-b76fdcd5933c",
   "metadata": {},
   "source": [
    "END OF LOGISTIC REGRESSION AND SCORING  \n",
    "BEGIN SUBSETTING (prior to KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7aa470d-0497-4737-87e1-037bd490e1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excuted at 2024-01-29 05:10:13\n"
     ]
    }
   ],
   "source": [
    "def generate_frequency_table(df, feature_combination):\n",
    "    return df.groupby(feature_combination).size().reset_index(name='Frequency')\n",
    "\n",
    "def subset_df_by_frequency_table(df, frequency_table):\n",
    "    subsets = []\n",
    "    for _, row in frequency_table.iterrows():\n",
    "        subset = df.copy()\n",
    "        for feature, value in zip(frequency_table.columns[:-1], row[:-1]):\n",
    "            subset = subset[subset[feature] == value]\n",
    "        subsets.append(subset)\n",
    "    return subsets\n",
    "executed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bfd543b-34f5-43b9-ac3e-01a782220d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excuted at 2024-01-29 05:10:13\n"
     ]
    }
   ],
   "source": [
    "frequency_table = generate_frequency_table(df=df, feature_combination=exact_match_features)\n",
    "subsets = subset_df_by_frequency_table(df=df, frequency_table=frequency_table)\n",
    "\n",
    "#del frequency_table\n",
    "executed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba8d906a-06b9-44be-9366-d0de04cb8afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CVD  Frequency\n",
      "0    0        255\n",
      "1    1         32\n",
      "   CVD  Frequency\n",
      "0    0         67\n",
      "1    1         46\n",
      "   CVD  Frequency\n",
      "0    0        188\n",
      "1    1         67\n",
      "   CVD  Frequency\n",
      "0    0        187\n",
      "1    1        158\n"
     ]
    }
   ],
   "source": [
    "for x in subsets:\n",
    "    print(x.groupby([binary_response]).size().reset_index(name='Frequency'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9d46645-ab5b-4c05-8517-5aa38058724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_knn(df, member_identifier):\n",
    "    # Create an empty set to store unique 'mb_minority' and 'mb_majority' values\n",
    "    unique_mb_minority = set()\n",
    "    unique_mb_majority = set()\n",
    "\n",
    "    # Create a list to store the indices to be dropped\n",
    "    indices_to_drop = []\n",
    "\n",
    "    # Iterate through the rows of the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        mb_minority = row[member_identifier+'_min']\n",
    "        mb_majority = row[member_identifier+'_maj']\n",
    "\n",
    "        # Check if 'mb_minority' or 'mb_majority' is not in the respective set\n",
    "        if mb_minority not in unique_mb_minority and mb_majority not in unique_mb_majority:\n",
    "            # If neither 'mb_minority' nor 'mb_majority' have occurred before, add them to the sets\n",
    "            unique_mb_minority.add(mb_minority)\n",
    "            unique_mb_majority.add(mb_majority)\n",
    "        else:\n",
    "            # If either 'mb_minority' or 'mb_majority' has occurred before, mark the index to be dropped\n",
    "            indices_to_drop.append(index)\n",
    "\n",
    "    # Drop the marked indices from the DataFrame\n",
    "    df.drop(indices_to_drop, inplace=True)\n",
    "\n",
    "    # Reset the index of the resulting DataFrame\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "372f39c5-ea32-46bb-817a-7be7a1c0033f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excuted at 2024-01-29 05:10:15\n"
     ]
    }
   ],
   "source": [
    "unique_knn_list=[]\n",
    "\n",
    "for exact_subset_df in subsets:\n",
    "    std_dev_score = exact_subset_df['logit'].std()\n",
    "    caliper = user_caliper if user_caliper is not None else 0.2 * std_dev_score\n",
    "    del std_dev_score\n",
    "    df_status_0 = exact_subset_df[exact_subset_df[binary_response] == 0][[member_identifier,'PropensityScore','logit','key']]\n",
    "    df_status_1 = exact_subset_df[exact_subset_df[binary_response] == 1][[member_identifier,'PropensityScore','logit','key']]\n",
    "\n",
    "    if len(df_status_1) <= len(df_status_0):\n",
    "        minority=df_status_1 \n",
    "        majority=df_status_0\n",
    "    else:\n",
    "        minority=df_status_0 \n",
    "        majority=df_status_1 \n",
    "    subset_result=majority.merge(minority,on='key',suffixes=('_maj','_min')).drop('key',axis=1)\n",
    "    del majority\n",
    "    del minority\n",
    "    subset_result['euclidean'] = ((subset_result['PropensityScore_min'] - subset_result['PropensityScore_maj'])**2)**0.5\n",
    "    subset_result['caliper']=caliper\n",
    "    subset_result = subset_result[[member_identifier+'_min', member_identifier+'_maj', 'PropensityScore_min', 'PropensityScore_maj', 'logit_min','logit_maj','euclidean','caliper']].query('euclidean < caliper').sort_values(by=[member_identifier+'_min', 'euclidean'], ascending=[True, True])\n",
    "    unique_knn_list.append(unique_knn(df=subset_result, member_identifier=member_identifier))\n",
    "    del subset_result\n",
    "executed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e40e16da-98b3-4df9-be70-d27953696ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excuted at 2024-01-29 05:10:15\n"
     ]
    }
   ],
   "source": [
    "unique_knn = pd.concat(unique_knn_list, ignore_index=True)\n",
    "#del unique_knn_list\n",
    "executed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e7b585d-e778-46f0-8639-fa75b303802c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excuted at 2024-01-29 05:10:15\n"
     ]
    }
   ],
   "source": [
    "unique_knn['match_id']= range(1, len(unique_knn) + 1) \n",
    "lkup_min = unique_knn[[member_identifier+'_min', 'match_id']].rename(columns={member_identifier+'_min': member_identifier})\n",
    "lkup_maj = unique_knn[[member_identifier+'_maj', 'match_id']].rename(columns={member_identifier+'_maj': member_identifier})\n",
    "executed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35e4168b-c3d5-4b32-a463-26641d01d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "lkup = pd.concat([lkup_min, lkup_maj]).sort_values(by=['match_id', member_identifier], ascending=[True, True]).reset_index(drop=True)\n",
    "lkup['match_ind']=1\n",
    "#del lkup_min\n",
    "#del lkup_maj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77ea7b91-1dc0-4af4-baf7-63461ed70682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 'df' with 'lkup' using a left join to retain all rows in 'df'\n",
    "df = pd.merge(df, lkup, on= member_identifier, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c289bb0-8382-4841-b5fe-f47083dfc1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  x.Age  x.Gender  Smoke  CVD  PropensityScore     logit  key  \\\n",
      "0           0   52.0         0      1    1         0.569969  0.281723    1   \n",
      "1           1   43.0         0      1    1         0.129403 -1.906250    1   \n",
      "2           2   55.0         0      0    0         0.076879 -2.485521    1   \n",
      "3           3   68.0         0      0    1         0.662596  0.674884    1   \n",
      "4           4   41.0         0      1    0         0.083749 -2.392466    1   \n",
      "\n",
      "   match_id  match_ind  \n",
      "0      33.0        1.0  \n",
      "1      34.0        1.0  \n",
      "2       NaN        NaN  \n",
      "3       1.0        1.0  \n",
      "4      61.0        1.0  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6b1ff1d-b8d9-4ea0-9f75-af92dc6a973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('p010_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95517d87-97c2-4c06-8756-5396a7c97d6c",
   "metadata": {},
   "source": [
    "SMD (Cohen's d) = (Mean of Group 1 - Mean of Group 2) / Pooled Standard Deviation\n",
    "\n",
    "Where:\n",
    "\n",
    "Mean of Group 1: The mean (average) of the variable in the first group.\n",
    "Mean of Group 2: The mean (average) of the variable in the second group.\n",
    "Pooled Standard Deviation: The standard deviation that considers both groups.\n",
    "The pooled standard deviation can be calculated as:\n",
    "\n",
    "Pooled Standard Deviation = sqrt[((n1 - 1) * Std Dev of Group 1^2 + (n2 - 1) * Std Dev of Group 2^2) / (n1 + n2 - 2)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
